{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BASE_URL = \"https://sleepdata.org/datasets/nchsdb/files/sleep_data\"\n",
    "SAVE_DIR = \"/tf/rahman/NCH_Sleep_Data_Bank/data/\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Paste your current session cookie here\n",
    "SESSION_COOKIE = \"_www_sleepdata_org_session=hyEH%2BitP3XzDC9biMVf67HqNeooCr8ROmAWVTvFPIE33vsxezroKOIrkeVMd%2BrWKf%2BDDG%2BLiD1aR0asaO%2FEzLpx22Kdu8T77Ojy%2FzuY7XhAYeHjFxF1zJVvzIdcht2BaGORNX4XlaiYwajK0LZrzh4DDFJdglTJ6bpbF4mpd%2Barjb%2FirlRgr8V65pRJvL9aSRCeX5OoXdnOUQMGJHuvE5rWCYt%2FA3IeUf982xiLb%2B3%2FvOrLqqzBzohT4Sjq%2B8cuFGMH6i1qWnoaPCOx0XZDB4Uw7m8udjAv5GwIgjNf%2FfU1XJDXDmyLZP%2BKLjLPP9gznICLYJldx5cNK6xnIBHqfSKdzJL5y%2FUh4lYUPL%2FW5LEAZmIJiG1DHEMeW69Ni5Al52hkhMJ9b2N1dImibNAx0HB2GVROTeNq1Zqq9jI6U%2F1bgSQR6444cwAB7E3i914AgJ6C92cMJi5OXz6Btw717uGRt8%2F%2BhgpZTgHRTkV05x1hFLIIuq%2B%2FT7rF7VT2j%2Bc3JtAjs7KhzdyjiC6vzbnBaBKI92DvxSw8LyBt1L%2BlAQytjCVnF11a2zi9rRW3lwPhtpQvgDbDy9ppErEq%2BvA1xrC3oy3qATiR5u5h2VOiYJcZw4lTp8cnQ3xmDACW9WSIMfm%2BB1OJA5PCuNd2jW%2FCD5ypZrsyIN%2FRX7zGn05hr82zCOO6H9G59zU28GLoWnfgooGhjLwnB%2FwMgdMzTrTcWYm3BaS9rWN%2F7%2FjXOv2VqQgStnrWK8YCx%2FPoapJWV1v%2F3a95lxfeb--VpWBlndzTmgsRb9J--ZiqyJkT%2Bha5DyLgr01W99A%3D%3D\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Referer\": \"https://sleepdata.org\",\n",
    "    \"Cookie\": \"_www_sleepdata_org_session=hyEH%2BitP3XzDC9biMVf67HqNeooCr8ROmAWVTvFPIE33vsxezroKOIrkeVMd%2BrWKf%2BDDG%2BLiD1aR0asaO%2FEzLpx22Kdu8T77Ojy%2FzuY7XhAYeHjFxF1zJVvzIdcht2BaGORNX4XlaiYwajK0LZrzh4DDFJdglTJ6bpbF4mpd%2Barjb%2FirlRgr8V65pRJvL9aSRCeX5OoXdnOUQMGJHuvE5rWCYt%2FA3IeUf982xiLb%2B3%2FvOrLqqzBzohT4Sjq%2B8cuFGMH6i1qWnoaPCOx0XZDB4Uw7m8udjAv5GwIgjNf%2FfU1XJDXDmyLZP%2BKLjLPP9gznICLYJldx5cNK6xnIBHqfSKdzJL5y%2FUh4lYUPL%2FW5LEAZmIJiG1DHEMeW69Ni5Al52hkhMJ9b2N1dImibNAx0HB2GVROTeNq1Zqq9jI6U%2F1bgSQR6444cwAB7E3i914AgJ6C92cMJi5OXz6Btw717uGRt8%2F%2BhgpZTgHRTkV05x1hFLIIuq%2B%2FT7rF7VT2j%2Bc3JtAjs7KhzdyjiC6vzbnBaBKI92DvxSw8LyBt1L%2BlAQytjCVnF11a2zi9rRW3lwPhtpQvgDbDy9ppErEq%2BvA1xrC3oy3qATiR5u5h2VOiYJcZw4lTp8cnQ3xmDACW9WSIMfm%2BB1OJA5PCuNd2jW%2FCD5ypZrsyIN%2FRX7zGn05hr82zCOO6H9G59zU28GLoWnfgooGhjLwnB%2FwMgdMzTrTcWYm3BaS9rWN%2F7%2FjXOv2VqQgStnrWK8YCx%2FPoapJWV1v%2F3a95lxfeb--VpWBlndzTmgsRb9J--ZiqyJkT%2Bha5DyLgr01W99A%3D%3D\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "\n",
    "def extract_files_from_page(page_number):\n",
    "    url = f\"{BASE_URL}?page={page_number}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access page {page_number} (status: {response.status_code})\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    files = []\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "        if href.endswith(\".edf\") or href.endswith(\".annot\"):\n",
    "            file_name = href.split(\"/\")[-1]\n",
    "            files.append(file_name)\n",
    "\n",
    "    print(f\"Page {page_number}: Found {len(files)} files\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def download_file(file_name):\n",
    "    url = f\"{BASE_URL}/{file_name}\"\n",
    "    local_path = os.path.join(SAVE_DIR, file_name)\n",
    "\n",
    "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
    "        print(f\"{file_name} already exists and is non-empty, skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading: {file_name}\")\n",
    "    try:\n",
    "        with requests.get(url, headers=HEADERS, stream=True, timeout=300) as res:\n",
    "            if res.status_code == 200:\n",
    "                with open(local_path, \"wb\") as f:\n",
    "                    for chunk in res.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "\n",
    "                downloaded_size = os.path.getsize(local_path)\n",
    "                print(f\"{file_name} downloaded successfully. Size: {downloaded_size / (1024*1024):.2f} MB\")\n",
    "            else:\n",
    "                print(f\"Failed to download {file_name} (status: {res.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {file_name}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    all_files = []\n",
    "\n",
    "    for page_num in range(0, 3):  # You can change this range based on number of pages\n",
    "        page_files = extract_files_from_page(page_num)\n",
    "        all_files.extend(page_files)\n",
    "\n",
    "    print(f\"Total files to download: {len(all_files)}\")\n",
    "\n",
    "    for file_name in set(all_files):  # remove duplicates\n",
    "        download_file(file_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
